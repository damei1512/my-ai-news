import os
import json
import datetime
import feedparser
import google.generativeai as genai
import time
import pytz
import re
import hashlib
from urllib.parse import urlparse

# ================= é…ç½®åŒº =================
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("âŒ API Key æœªé…ç½®")

genai.configure(api_key=GEMINI_API_KEY)
MODEL_NAME = 'gemini-flash-latest'

# ================= RSS æºé…ç½® =================
RSS_SOURCES = {
    "ç§‘æŠ€": [
        "https://36kr.com/feed",
        "https://www.ifanr.com/feed",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
        "https://www.pingwest.com/feed",
        "https://www.jiqizhixin.com/rss",
    ],
    "æ•°ç ": [
        "https://www.engadget.com/rss.xml",
        "https://www.ifanr.com/feed",
    ],
    "æ¸¸æˆ": [
        "https://www.ign.com/rss/articles/feed",
        "https://www.gamespot.com/feeds/news/",
        "https://www.gcores.com/rss",
    ],
    "æ—¶äº‹": [
        "https://feeds.bbci.co.uk/news/world/rss.xml",
        "https://www.reutersagency.com/feed/?taxonomy=markets&post_type=reuters-best",
    ],
    "AI": [
        "https://openai.com/index/rss.xml",
        "https://www.anthropic.com/rss.xml",
        "https://www.wired.com/feed/tag/ai/latest/rss",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
    ]
}

# ================= å…³é”®è¯ç™½åå• =================
KEYWORD_WHITELIST = {
    "ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "èèµ„", "IPO", "æ”¶è´­", "ä¸Šå¸‚", "è‹¹æœ", "è°·æ­Œ", "å¾®è½¯", "è‹±ä¼Ÿè¾¾", "åä¸º", "å°ç±³", "ç‰¹æ–¯æ‹‰", "SpaceX", "OpenAI", "Anthropic", "AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "å…·èº«æ™ºèƒ½", "æœºå™¨äºº", "è‡ªåŠ¨é©¾é©¶", "ç”µåŠ¨è½¦", "æ–°èƒ½æº", "ç®—åŠ›", "äº‘æœåŠ¡", "å¤§ç–†", "æ¯”äºšè¿ª", "è”šæ¥", "ç†æƒ³", "å°é¹"],
    "æ•°ç ": ["æ‰‹æœº", "ç›¸æœº", "ç¬”è®°æœ¬", "å¹³æ¿", "æ‰‹è¡¨", "è€³æœº", "è¯„æµ‹", "ä½“éªŒ", "å‘å¸ƒ", "iPhone", "Android", "æ‘„å½±", "å°ç±³", "åä¸º", "OPPO", "vivo", "ä¸‰æ˜Ÿ", "ç´¢å°¼", "ä½³èƒ½", "å°¼åº·", "GoPro", "æ— äººæœº", "é…ä»¶", "å……ç”µ", "å±å¹•", "æ˜¾ç¤ºå™¨", "é”®ç›˜", "é¼ æ ‡"],
    "æ¸¸æˆ": ["Switch", "PlayStation", "Xbox", "Steam", "æ‰‹æ¸¸", "ç½‘æ¸¸", "DLC", "ä»»å¤©å ‚", "ç´¢å°¼", "å¾®è½¯", "é”€é‡", "å‘å”®", "åŸç¥", "ç‹è€…è£è€€", "é»‘ç¥è¯", "GTA", "å¡å°”è¾¾", "é©¬é‡Œå¥¥", "å®å¯æ¢¦", "ç”µç«", "CS", "LOL", "Dota", "æ›´æ–°", "é¢„å‘Š", "æ¼”ç¤º"],
    "æ—¶äº‹": ["ç»æµ", "æ”¿ç­–", "è´¸æ˜“", "å…³ç¨", "åˆ¶è£", "é€‰ä¸¾", "æˆ˜äº‰", "å†²çª", "ç–«æƒ…", "æ°”å€™å˜åŒ–", "ä¸­ç¾", "æ¬§ç›Ÿ", "ä¿„ç½—æ–¯", "ä¹Œå…‹å…°", "è‚¡å¸‚", "å¤®è¡Œ", "é€šèƒ€", "å°±ä¸š", "GDP", "ç§‘æŠ€æˆ˜", "æ‹œç™»", "ç‰¹æœ—æ™®", "é©¬å…‹é¾™", "å¾·å›½"],
    "AI": ["ChatGPT", "Claude", "Gemini", "Llama", "å¤§æ¨¡å‹", "LLM", "ç”Ÿæˆå¼AI", "AIGC", "ç®—åŠ›", "GPU", "Agent", "å¤šæ¨¡æ€", "AGI", "Prompt", "å¾®è°ƒ", "è®­ç»ƒ", "æ¨ç†", "OpenAI", "Anthropic", "Google", "Meta", "DeepSeek", "Perplexity", "Midjourney", "Sora", "AIè§†é¢‘", "AIå›¾ç‰‡", "AIéŸ³ä¹", "ä»£ç ç”Ÿæˆ"]
}

# ================= å»é‡ç®¡ç†å™¨ =================
class ArticleDeduplicator:
    """æ–‡ç« å»é‡ç®¡ç†å™¨ - æ”¯æŒé“¾æ¥å»é‡å’Œå†…å®¹ç›¸ä¼¼åº¦å»é‡"""
    
    def __init__(self):
        self.seen_links = set()  # å·²è§è¿‡çš„é“¾æ¥
        self.seen_hashes = set()  # å·²è§è¿‡çš„å†…å®¹æŒ‡çº¹
        self.removed_count = 0
    
    def normalize_link(self, link):
        """æ ‡å‡†åŒ–é“¾æ¥ï¼Œå»é™¤è·Ÿè¸ªå‚æ•°"""
        try:
            parsed = urlparse(link)
            # å»é™¤å¸¸è§çš„è·Ÿè¸ªå‚æ•°
            clean_link = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            return clean_link.rstrip('/')
        except:
            return link
    
    def content_fingerprint(self, title, summary):
        """ç”Ÿæˆå†…å®¹æŒ‡çº¹ç”¨äºç›¸ä¼¼åº¦æ£€æµ‹"""
        # æå–å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯åçš„æ ¸å¿ƒè¯ï¼‰
        text = (title + ' ' + summary).lower()
        # å»é™¤å¸¸è§åœç”¨è¯å’Œæ ‡ç‚¹
        text = re.sub(r'[ï¼Œã€‚ï¼Ÿï¼.,?!;:"\'\s\d]+', ' ', text)
        # æå–2-3ä¸ªå­—ç¬¦çš„è¯ç»„ä½œä¸ºç‰¹å¾
        words = [w for w in text.split() if len(w) >= 2]
        # å–å‰10ä¸ªå…³é”®è¯æ’åºåç”ŸæˆæŒ‡çº¹
        keywords = sorted(words)[:10]
        fingerprint = hashlib.md5(' '.join(keywords).encode()).hexdigest()[:16]
        return fingerprint
    
    def is_duplicate(self, article):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦é‡å¤"""
        link = article.get('link', '')
        title = article.get('title', '')
        summary = article.get('summary', '')
        
        # 1. é“¾æ¥å»é‡
        normalized = self.normalize_link(link)
        if normalized in self.seen_links:
            return True, "é“¾æ¥é‡å¤"
        
        # 2. æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼ˆæ ‡é¢˜å®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼ï¼‰
        normalized_title = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())
        for seen_link in self.seen_links:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ç»´æŠ¤æ ‡é¢˜ç´¢å¼•
            pass
        
        # 3. å†…å®¹æŒ‡çº¹å»é‡
        fingerprint = self.content_fingerprint(title, summary)
        if fingerprint in self.seen_hashes:
            return True, "å†…å®¹ç›¸ä¼¼"
        
        # è®°å½•ä¸ºæ–°æ–‡ç« 
        self.seen_links.add(normalized)
        self.seen_hashes.add(fingerprint)
        return False, None
    
    def deduplicate_list(self, articles):
        """å¯¹æ–‡ç« åˆ—è¡¨è¿›è¡Œå»é‡"""
        unique_articles = []
        duplicates = []
        
        for article in articles:
            is_dup, reason = self.is_duplicate(article)
            if is_dup:
                duplicates.append({
                    'title': article.get('title', '')[:50],
                    'reason': reason
                })
            else:
                unique_articles.append(article)
        
        self.removed_count = len(duplicates)
        if duplicates:
            print(f"   ğŸ§¹ å»é‡: ç§»é™¤ {len(duplicates)} æ¡é‡å¤")
            for d in duplicates[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - {d['title'][:40]}... ({d['reason']})")
            if len(duplicates) > 3:
                print(f"      ... è¿˜æœ‰ {len(duplicates)-3} æ¡")
        
        return unique_articles

# ================= å›¾ç‰‡æå–å™¨ =================
class ImageExtractor:
    """ä»RSSæå–é…å›¾ï¼ˆåŸæ–‡æœ‰å›¾æ‰ç”¨ï¼Œæ— å›¾ä¸ç¡¬é…ï¼‰"""
    
    @staticmethod
    def extract_from_entry(entry):
        """ä»RSS entryæå–å›¾ç‰‡URL"""
        # 1. å°è¯•è·å–media:content
        if 'media_content' in entry:
            for media in entry.media_content:
                if media.get('type', '').startswith('image/'):
                    return media.get('url')
        
        # 2. å°è¯•è·å–media:thumbnail
        if 'media_thumbnail' in entry and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url')
        
        # 3. å°è¯•ä»summary/contentä¸­æå–imgæ ‡ç­¾
        content = entry.get('summary', '') + entry.get('description', '')
        img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content, re.IGNORECASE)
        if img_match:
            return img_match.group(1)
        
        # 4. å°è¯•enclosure
        if 'enclosures' in entry and entry.enclosures:
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image/'):
                    return enc.get('href')
        
        return None

# ================= æ ¸å¿ƒå‡½æ•° =================

def get_current_date_info():
    """è·å–åŒ—äº¬æ—¶é—´æ—¥æœŸå’Œæ˜ŸæœŸ"""
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz)
    date_str = now.strftime("%Y-%m-%d")
    week_map = {0: "å‘¨ä¸€", 1: "å‘¨äºŒ", 2: "å‘¨ä¸‰", 3: "å‘¨å››", 4: "å‘¨äº”", 5: "å‘¨å…­", 6: "å‘¨æ—¥"}
    week_str = week_map[now.weekday()]
    return date_str, week_str

def filter_by_keywords(articles, category):
    """æŒ‰å…³é”®è¯è¿‡æ»¤æ–‡ç« """
    keywords = KEYWORD_WHITELIST.get(category, [])
    if not keywords:
        return articles
    
    filtered = []
    for article in articles:
        text = (article.get('title', '') + ' ' + article.get('summary', '')).lower()
        if any(kw.lower() in text for kw in keywords):
            filtered.append(article)
    
    if len(filtered) < len(articles):
        print(f"   ğŸ“ å…³é”®è¯è¿‡æ»¤: {len(articles)} â†’ {len(filtered)} ç¯‡")
    
    return filtered

def fetch_news_by_category(category, urls, deduplicator):
    """æŠ“å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»ï¼ˆå«å›¾ç‰‡ï¼‰"""
    print(f"ğŸ“¡ [{category}] æ­£åœ¨æŠ“å– RSS æº...")
    articles = []
    
    for url in urls:
        try:
            feed = feedparser.parse(url)
            print(f"   âœ“ {url.split('/')[2]}")  # åªæ˜¾ç¤ºåŸŸå
            
            for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–å‰5æ¡
                # æå–å›¾ç‰‡
                image_url = ImageExtractor.extract_from_entry(entry)
                
                article = {
                    "title": entry.title,
                    "link": entry.link,
                    "summary": entry.get('summary', '')[:500],
                    "image": image_url,
                    "source_domain": url.split('/')[2]
                }
                articles.append(article)
                
        except Exception as e:
            print(f"   âŒ {url} - {e}")
    
    # å»é‡
    articles = deduplicator.deduplicate_list(articles)
    
    # å…³é”®è¯è¿‡æ»¤
    articles = filter_by_keywords(articles, category)
    
    return articles

def summarize_with_gemini(category, articles):
    """ä½¿ç”¨ Gemini å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»æ€»ç»“ï¼ˆå«å›¾ç‰‡ä¿¡æ¯ï¼‰"""
    if not articles:
        return []
    
    print(f"ğŸ¤– [{category}] æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        
        # æ„å»ºå¸¦å›¾ç‰‡ä¿¡æ¯çš„å†…å®¹
        content_parts = []
        for i, a in enumerate(articles, 1):
            img_info = f"[é…å›¾: {a.get('image', 'æ— ')}]" if a.get('image') else "[æ— é…å›¾]"
            content_parts.append(
                f"[{i}] æ ‡é¢˜: {a['title']}\n"
                f"é“¾æ¥: {a['link']}\n"
                f"{img_info}\n"
                f"ç®€ä»‹: {a['summary'][:300]}"
            )
        
        content = "\n\n---\n\n".join(content_parts)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ä¸»ç¼–ã€‚è¯·å°†ä»¥ä¸‹{category}ç±»æ–°é—»ç”Ÿæˆä¸ºä¸­æ–‡æ—¥æŠ¥æ‘˜è¦ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. category å­—æ®µå¿…é¡»å¡« "{category}"
2. tag å­—æ®µå¡«å†™æ–°é—»çš„å­æ ‡ç­¾ï¼ˆå¦‚å¤§æ¨¡å‹ã€èŠ¯ç‰‡ã€æ¸¸æˆç­‰ï¼Œ3-4ä¸ªå­—ï¼‰
3. ä¿ç•™åŸæ–‡ link
4. summary å­—æ®µï¼š100å­—ä»¥å†…ï¼Œçªå‡ºæ ¸å¿ƒä¿¡æ¯
5. comment å­—æ®µï¼šæ¯’èˆŒç‚¹è¯„ï¼Œ50å­—ä»¥å†…ï¼Œè¦æœ‰æ€åº¦
6. image å­—æ®µï¼šä¿ç•™åŸæ–‡ä¸­çš„é…å›¾URLï¼Œå¦‚æœæ²¡æœ‰åˆ™ç•™ç©ºå­—ç¬¦ä¸²
7. è¾“å‡ºçº¯ JSON åˆ—è¡¨ï¼Œæ—  Markdown

JSON æ ¼å¼ç¤ºä¾‹ï¼š
[
    {{
        "category": "{category}",
        "tag": "å¤§æ¨¡å‹",
        "title": "ä¸­æ–‡æ ‡é¢˜",
        "link": "https://...",
        "summary": "ä¸­æ–‡æ‘˜è¦...",
        "comment": "æ¯’èˆŒç‚¹è¯„...",
        "image": "https://..." æˆ– ""
    }}
]

æ–°é—»å†…å®¹ï¼š
{content}
"""
        
        time.sleep(1)
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # æ¸…ç† Markdown ä»£ç å—
        if text.startswith("```json"): text = text[7:]
        if text.startswith("```"): text = text[3:]
        if text.endswith("```"): text = text[:-3]
        
        result = json.loads(text)
        
        # ç¡®ä¿æ¯æ¡æ–°é—»éƒ½æœ‰imageå­—æ®µ
        for item in result:
            if 'image' not in item:
                item['image'] = ""
        
        return result
        
    except Exception as e:
        print(f"âŒ [{category}] API é”™è¯¯: {e}")
        return []

if __name__ == "__main__":
    today_date, today_week = get_current_date_info()
    history_file = 'news.json'
    
    # åŠ è½½å†å²æ•°æ®
    archive_data = {}
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                archive_data = json.load(f)
        except:
            pass

    all_articles = []
    deduplicator = ArticleDeduplicator()
    
    # ========== 1. æŠ“å– RSS æº ==========
    print("=" * 50)
    print("ğŸ“¡ é˜¶æ®µ1: æŠ“å– RSS æº")
    print("=" * 50)
    
    for category, urls in RSS_SOURCES.items():
        raw_news = fetch_news_by_category(category, urls, deduplicator)
        if raw_news:
            summarized = summarize_with_gemini(category, raw_news)
            all_articles.extend(summarized)
        time.sleep(1)
    
    # ========== 2. æœ€ç»ˆå»é‡ï¼ˆè·¨åˆ†ç±»ï¼‰==========
    print("\n" + "=" * 50)
    print("ğŸ§¹ é˜¶æ®µ2: è·¨åˆ†ç±»å»é‡")
    print("=" * 50)
    
    final_deduplicator = ArticleDeduplicator()
    all_articles = final_deduplicator.deduplicate_list(all_articles)
    
    # ========== 3. é…å›¾ç»Ÿè®¡ ==========
    print("\n" + "=" * 50)
    print("ğŸ–¼ï¸ é˜¶æ®µ3: é…å›¾ç»Ÿè®¡")
    print("=" * 50)
    
    with_image = sum(1 for a in all_articles if a.get('image'))
    print(f"   åŸæ–‡é…å›¾: {with_image}/{len(all_articles)} æ¡")
    
    # ========== 4. æŠ“å– X (Twitter) å¤§V ==========
    print("\n" + "=" * 50)
    print("ğŸ¦ é˜¶æ®µ4: æŠ“å– X (Twitter) å¤§V")
    print("=" * 50)
    
    try:
        from x_fetcher import fetch_all_x_tweets
        x_articles = fetch_all_x_tweets()
        if x_articles:
            print(f"\nğŸ¤– æ­£åœ¨å¤„ç† {len(x_articles)} æ¡ X æ¨æ–‡...")
            for article in x_articles:
                article['comment'] = f"ã€{article.get('source_name', 'X')} æœ€æ–°åŠ¨æ€ã€‘"
            all_articles.extend(x_articles)
    except Exception as e:
        print(f"âš ï¸ X æŠ“å–å¤±è´¥: {e}")
    
    # ========== 5. ä¿å­˜æ•°æ® ==========
    print("\n" + "=" * 50)
    print("ğŸ’¾ é˜¶æ®µ5: ä¿å­˜æ•°æ®")
    print("=" * 50)
    
    if all_articles:
        archive_data[today_date] = {
            "week": today_week,
            "articles": all_articles
        }
        print(f"âœ… ä»Šæ—¥å…± {len(all_articles)} æ¡æ–°é—»ï¼ˆå·²å»é‡ï¼‰")
        
        # åˆ†ç±»ç»Ÿè®¡
        from collections import Counter
        cat_stats = Counter([a.get('category', 'æœªçŸ¥') for a in all_articles])
        print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        for cat, count in cat_stats.most_common():
            print(f"   {cat}: {count}æ¡")
    
    # 7å¤©æ»šåŠ¨æ¸…æ´—
    sorted_dates = sorted(archive_data.keys(), reverse=True)
    final_data = {d: archive_data[d] for d in sorted_dates[:7]}
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæˆï¼å·²ä¿å­˜åˆ° {history_file}")
    print(f"ğŸ§¹ æ€»è®¡å»é‡: {deduplicator.removed_count + final_deduplicator.removed_count} æ¡")
