import os
import json
import datetime
import feedparser
import google.generativeai as genai
import time
import pytz

# ================= é…ç½®åŒº =================
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("âŒ API Key æœªé…ç½®")

genai.configure(api_key=GEMINI_API_KEY)
MODEL_NAME = 'gemini-flash-latest'

# ================= RSS æºé…ç½® =================
# æŒ‰åˆ†ç±»ç»„ç»‡çš„ RSS æº
RSS_SOURCES = {
    "ç§‘æŠ€": [
        "https://36kr.com/feed",
        "https://www.ifanr.com/feed",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
        "https://www.pingwest.com/feed",  # å“ç©
        "https://www.jiqizhixin.com/rss", # æœºå™¨ä¹‹å¿ƒ
    ],
    "æ•°ç ": [
        "https://www.engadget.com/rss.xml",
        "https://www.ifanr.com/feed",
    ],
    "æ¸¸æˆ": [
        "https://www.ign.com/rss/articles/feed",
        "https://www.gamespot.com/feeds/news/",
        "https://www.gcores.com/rss",  # æœºæ ¸
    ],
    "æ—¶äº‹": [
        "https://feeds.bbci.co.uk/news/world/rss.xml",
        "https://www.reutersagency.com/feed/?taxonomy=markets&post_type=reuters-best",
    ],
    "AI": [
        "https://openai.com/index/rss.xml",
        "https://www.anthropic.com/rss.xml",
        "https://www.wired.com/feed/tag/ai/latest/rss",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
    ]
}

# ================= å…³é”®è¯ç™½åå• =================
# åªæœ‰æ ‡é¢˜/æ‘˜è¦åŒ…å«è¿™äº›å…³é”®è¯çš„æ–‡ç« æ‰ä¼šä¿ç•™
KEYWORD_WHITELIST = {
    "ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "èèµ„", "IPO", "æ”¶è´­", "ä¸Šå¸‚", "è‹¹æœ", "è°·æ­Œ", "å¾®è½¯", "è‹±ä¼Ÿè¾¾", "åä¸º", "å°ç±³", "ç‰¹æ–¯æ‹‰", "SpaceX", "OpenAI", "Anthropic", "AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹"],
    "æ•°ç ": ["æ‰‹æœº", "ç›¸æœº", "ç¬”è®°æœ¬", "å¹³æ¿", "æ‰‹è¡¨", "è€³æœº", "è¯„æµ‹", "ä½“éªŒ", "å‘å¸ƒ", "iPhone", "Android", "æ‘„å½±"],
    "æ¸¸æˆ": ["Switch", "PlayStation", "Xbox", "Steam", "æ‰‹æ¸¸", "ç½‘æ¸¸", "DLC", "ä»»å¤©å ‚", "ç´¢å°¼", "å¾®è½¯", "é”€é‡", "å‘å”®"],
    "æ—¶äº‹": ["ç»æµ", "æ”¿ç­–", "è´¸æ˜“", "å…³ç¨", "åˆ¶è£", "é€‰ä¸¾", "æˆ˜äº‰", "å†²çª", "ç–«æƒ…", "æ°”å€™å˜åŒ–"],
    "AI": ["ChatGPT", "Claude", "Gemini", "Llama", "å¤§æ¨¡å‹", "LLM", "ç”Ÿæˆå¼AI", "AIGC", "ç®—åŠ›", "GPU", "Agent", "å¤šæ¨¡æ€", "AGI", "Prompt", "å¾®è°ƒ", "è®­ç»ƒ"]
}

def filter_by_keywords(articles, category):
    """æŒ‰å…³é”®è¯è¿‡æ»¤æ–‡ç« """
    keywords = KEYWORD_WHITELIST.get(category, [])
    if not keywords:
        return articles
    
    filtered = []
    for article in articles:
        text = (article.get('title', '') + ' ' + article.get('summary', '')).lower()
        if any(kw.lower() in text for kw in keywords):
            filtered.append(article)
    
    # è®°å½•è¿‡æ»¤ä¿¡æ¯
    if len(filtered) < len(articles):
        print(f"   ğŸ“ å…³é”®è¯è¿‡æ»¤: {len(articles)} â†’ {len(filtered)} ç¯‡")
    
    return filtered

# ================= æ ¸å¿ƒé€»è¾‘ =================

def get_current_date_info():
    """è·å–åŒ—äº¬æ—¶é—´æ—¥æœŸå’Œæ˜ŸæœŸ"""
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz)
    date_str = now.strftime("%Y-%m-%d")
    week_map = {0: "å‘¨ä¸€", 1: "å‘¨äºŒ", 2: "å‘¨ä¸‰", 3: "å‘¨å››", 4: "å‘¨äº”", 5: "å‘¨å…­", 6: "å‘¨æ—¥"}
    week_str = week_map[now.weekday()]
    return date_str, week_str

def fetch_news_by_category(category, urls):
    """æŠ“å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»"""
    print(f"ğŸ“¡ [{category}] æ­£åœ¨æŠ“å– RSS æº...")
    articles = []
    for url in urls:
        try:
            feed = feedparser.parse(url)
            print(f"   âœ“ {url}")
            for entry in feed.entries[:3]:  # æ¯ä¸ªæºå–å‰3æ¡
                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "summary": entry.get('summary', '')[:300]
                })
        except Exception as e:
            print(f"   âŒ {url} - {e}")
    
    # å…³é”®è¯è¿‡æ»¤
    articles = filter_by_keywords(articles, category)
    return articles

def summarize_with_gemini(category, articles):
    """ä½¿ç”¨ Gemini å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»æ€»ç»“"""
    if not articles:
        return []
    
    print(f"ğŸ¤– [{category}] æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        
        content = "\n\n---\n\n".join([
            f"æ ‡é¢˜: {a['title']}\né“¾æ¥: {a['link']}\nç®€ä»‹: {a['summary']}"
            for a in articles
        ])
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ä¸»ç¼–ã€‚è¯·å°†ä»¥ä¸‹{category}ç±»æ–°é—»ç”Ÿæˆä¸ºä¸­æ–‡æ—¥æŠ¥æ‘˜è¦ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. category å­—æ®µå¿…é¡»å¡« "{category}"
2. tag å­—æ®µå¡«å†™æ–°é—»çš„å­æ ‡ç­¾ï¼ˆå¦‚å¤§æ¨¡å‹ã€èŠ¯ç‰‡ã€æ¸¸æˆç­‰ï¼‰
3. ä¿ç•™åŸæ–‡ Link
4. è¾“å‡ºçº¯ JSON åˆ—è¡¨ï¼Œæ—  Markdown

JSON æ ¼å¼ç¤ºä¾‹ï¼š
[
    {{
        "category": "{category}",
        "tag": "å­æ ‡ç­¾",
        "title": "ä¸­æ–‡æ ‡é¢˜",
        "link": "https://...",
        "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰",
        "comment": "æ¯’èˆŒç‚¹è¯„ï¼ˆ50å­—ä»¥å†…ï¼‰"
    }}
]

æ–°é—»å†…å®¹ï¼š
{content}
"""
        
        time.sleep(1)
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # æ¸…ç† Markdown ä»£ç å—
        if text.startswith("```json"): text = text[7:]
        if text.startswith("```"): text = text[3:]
        if text.endswith("```"): text = text[:-3]
        
        return json.loads(text)
        
    except Exception as e:
        print(f"âŒ [{category}] API é”™è¯¯: {e}")
        return []

if __name__ == "__main__":
    today_date, today_week = get_current_date_info()
    history_file = 'news.json'
    
    # åŠ è½½å†å²æ•°æ®
    archive_data = {}
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                archive_data = json.load(f)
        except:
            pass

    all_articles = []
    
    # ========== 1. æŠ“å– RSS æº ==========
    print("=" * 50)
    print("ğŸ“¡ é˜¶æ®µ1: æŠ“å– RSS æº")
    print("=" * 50)
    
    for category, urls in RSS_SOURCES.items():
        raw_news = fetch_news_by_category(category, urls)
        if raw_news:
            summarized = summarize_with_gemini(category, raw_news)
            all_articles.extend(summarized)
        time.sleep(1)
    
    # ========== 2. æŠ“å–å¾®åšå¤§V ==========
    print("\n" + "=" * 50)
    print("ğŸ“± é˜¶æ®µ2: æŠ“å–å¾®åšå¤§V")
    print("=" * 50)
    
    try:
        from weibo_fetcher import fetch_all_weibo
        weibo_articles = fetch_all_weibo()
        if weibo_articles:
            # å¯¹å¾®åšå†…å®¹ä¹ŸåšAIæ€»ç»“
            print("\nğŸ¤– æ­£åœ¨æ€»ç»“å¾®åšå†…å®¹...")
            for article in weibo_articles:
                # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ç”¨åŸæ–‡ï¼ŒåŠ AIç‚¹è¯„
                article['comment'] = f"ã€{article['source_name']}å¾®åšã€‘å¤§ä½¬å‘è¯"
            all_articles.extend(weibo_articles)
    except Exception as e:
        print(f"âš ï¸ å¾®åšæŠ“å–å¤±è´¥: {e}")
    
    # ========== 3. ä¿å­˜æ•°æ® ==========
    print("\n" + "=" * 50)
    print("ğŸ’¾ é˜¶æ®µ3: ä¿å­˜æ•°æ®")
    print("=" * 50)
    
    if all_articles:
        archive_data[today_date] = {
            "week": today_week,
            "articles": all_articles
        }
        print(f"âœ… ä»Šæ—¥å…± {len(all_articles)} æ¡æ–°é—»")
        
        # åˆ†ç±»ç»Ÿè®¡
        from collections import Counter
        cat_stats = Counter([a.get('category', 'æœªçŸ¥') for a in all_articles])
        print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        for cat, count in cat_stats.most_common():
            print(f"   {cat}: {count}æ¡")
    
    # 7å¤©æ»šåŠ¨æ¸…æ´—
    sorted_dates = sorted(archive_data.keys(), reverse=True)
    final_data = {d: archive_data[d] for d in sorted_dates[:7]}
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… å®Œæˆï¼å·²ä¿å­˜åˆ° {history_file}")
