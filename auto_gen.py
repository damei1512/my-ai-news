import os
import json
import datetime
import feedparser
import google.generativeai as genai
import time
import pytz

# ================= é…ç½®åŒº =================
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("âŒ API Key æœªé…ç½®")

genai.configure(api_key=GEMINI_API_KEY)
MODEL_NAME = 'gemini-flash-latest'

# ================= RSS æºé…ç½® =================
# æŒ‰åˆ†ç±»ç»„ç»‡çš„ RSS æº
RSS_SOURCES = {
    "ç§‘æŠ€": [
        "https://36kr.com/feed",
        "https://www.ifanr.com/feed",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
    ],
    "æ•°ç ": [
        "https://www.engadget.com/rss.xml",
        "https://www.ifanr.com/feed",
    ],
    "æ¸¸æˆ": [
        "https://www.ign.com/rss/articles/feed",
        "https://www.gamespot.com/feeds/news/",
    ],
    "æ—¶äº‹": [
        "https://feeds.bbci.co.uk/news/world/rss.xml",
        "https://www.reutersagency.com/feed/?taxonomy=markets&post_type=reuters-best",
    ],
    "AI": [
        "https://openai.com/index/rss.xml",
        "https://www.anthropic.com/rss.xml",
        "https://www.wired.com/feed/tag/ai/latest/rss",
        "https://techcrunch.com/category/artificial-intelligence/feed/",
    ]
}

# ================= æ ¸å¿ƒé€»è¾‘ =================

def get_current_date_info():
    """è·å–åŒ—äº¬æ—¶é—´æ—¥æœŸå’Œæ˜ŸæœŸ"""
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz)
    date_str = now.strftime("%Y-%m-%d")
    week_map = {0: "å‘¨ä¸€", 1: "å‘¨äºŒ", 2: "å‘¨ä¸‰", 3: "å‘¨å››", 4: "å‘¨äº”", 5: "å‘¨å…­", 6: "å‘¨æ—¥"}
    week_str = week_map[now.weekday()]
    return date_str, week_str

def fetch_news_by_category(category, urls):
    """æŠ“å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»"""
    print(f"ğŸ“¡ [{category}] æ­£åœ¨æŠ“å– RSS æº...")
    articles = []
    for url in urls:
        try:
            feed = feedparser.parse(url)
            print(f"   âœ“ {url}")
            for entry in feed.entries[:3]:  # æ¯ä¸ªæºå–å‰3æ¡
                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "summary": entry.get('summary', '')[:300]
                })
        except Exception as e:
            print(f"   âŒ {url} - {e}")
    return articles

def summarize_with_gemini(category, articles):
    """ä½¿ç”¨ Gemini å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»æ€»ç»“"""
    if not articles:
        return []
    
    print(f"ğŸ¤– [{category}] æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        
        content = "\n\n---\n\n".join([
            f"æ ‡é¢˜: {a['title']}\né“¾æ¥: {a['link']}\nç®€ä»‹: {a['summary']}"
            for a in articles
        ])
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ä¸»ç¼–ã€‚è¯·å°†ä»¥ä¸‹{category}ç±»æ–°é—»ç”Ÿæˆä¸ºä¸­æ–‡æ—¥æŠ¥æ‘˜è¦ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. category å­—æ®µå¿…é¡»å¡« "{category}"
2. tag å­—æ®µå¡«å†™æ–°é—»çš„å­æ ‡ç­¾ï¼ˆå¦‚å¤§æ¨¡å‹ã€èŠ¯ç‰‡ã€æ¸¸æˆç­‰ï¼‰
3. ä¿ç•™åŸæ–‡ Link
4. è¾“å‡ºçº¯ JSON åˆ—è¡¨ï¼Œæ—  Markdown

JSON æ ¼å¼ç¤ºä¾‹ï¼š
[
    {{
        "category": "{category}",
        "tag": "å­æ ‡ç­¾",
        "title": "ä¸­æ–‡æ ‡é¢˜",
        "link": "https://...",
        "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰",
        "comment": "æ¯’èˆŒç‚¹è¯„ï¼ˆ50å­—ä»¥å†…ï¼‰"
    }}
]

æ–°é—»å†…å®¹ï¼š
{content}
"""
        
        time.sleep(1)
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # æ¸…ç† Markdown ä»£ç å—
        if text.startswith("```json"): text = text[7:]
        if text.startswith("```"): text = text[3:]
        if text.endswith("```"): text = text[:-3]
        
        return json.loads(text)
        
    except Exception as e:
        print(f"âŒ [{category}] API é”™è¯¯: {e}")
        return []

if __name__ == "__main__":
    today_date, today_week = get_current_date_info()
    history_file = 'news.json'
    
    # åŠ è½½å†å²æ•°æ®
    archive_data = {}
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                archive_data = json.load(f)
        except:
            pass

    # æŒ‰åˆ†ç±»æŠ“å–å’Œç”Ÿæˆ
    all_articles = []
    for category, urls in RSS_SOURCES.items():
        raw_news = fetch_news_by_category(category, urls)
        if raw_news:
            summarized = summarize_with_gemini(category, raw_news)
            all_articles.extend(summarized)
        time.sleep(1)  # é¿å… API é™æµ
    
    # ä¿å­˜ä»Šæ—¥æ•°æ®
    if all_articles:
        archive_data[today_date] = {
            "week": today_week,
            "articles": all_articles
        }
        print(f"âœ… å·²ç”Ÿæˆ {len(all_articles)} æ¡æ–°é—»")
    
    # 7å¤©æ»šåŠ¨æ¸…æ´—
    sorted_dates = sorted(archive_data.keys(), reverse=True)
    final_data = {d: archive_data[d] for d in sorted_dates[:7]}
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… å®Œæˆï¼å·²ä¿å­˜åˆ° {history_file}")
